{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, List\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "import nest_asyncio\n",
    "import logging\n",
    "from collections import Counter\n",
    "import traceback\n",
    "\n",
    "# Mount Google Drive and setup directories\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    base_dir = '/content/drive/MyDrive/Civitai_Data'\n",
    "    DATA_DIRS = {\n",
    "        'creators': os.path.join(base_dir, 'creators'),\n",
    "        'models': os.path.join(base_dir, 'models'),\n",
    "        'csv': os.path.join(base_dir, 'csv'),\n",
    "        'logs': os.path.join(base_dir, 'logs'),\n",
    "        'checkpoints': os.path.join(base_dir, 'checkpoints'),\n",
    "        'images': os.path.join(base_dir, 'images')\n",
    "    }\n",
    "\n",
    "    for dir_path in DATA_DIRS.values():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Failed to setup Google Drive and directories: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimitManager:\n",
    "    def __init__(self, max_requests: int = 100, time_window: int = 60):\n",
    "        self.max_requests = max_requests\n",
    "        self.time_window = time_window\n",
    "        self.requests = []\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def wait_if_needed(self):\n",
    "        \"\"\"Check and wait if rate limit is approaching\"\"\"\n",
    "        async with self.lock:\n",
    "            current_time = time.time()\n",
    "            self.requests = [t for t in self.requests if current_time - t < self.time_window]\n",
    "\n",
    "            if len(self.requests) >= self.max_requests * 0.8:\n",
    "                oldest_request = self.requests[0]\n",
    "                wait_time = self.time_window - (current_time - oldest_request)\n",
    "                if wait_time > 0:\n",
    "                    print(f\"Approaching rate limit. Waiting {wait_time:.2f} seconds...\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                    self.requests = []\n",
    "\n",
    "            self.requests.append(current_time)\n",
    "\n",
    "class ProgressTracker:\n",
    "    def __init__(self, checkpoint_dir: str):\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, 'crawler_progress.json')\n",
    "        self.progress = self.load_progress()\n",
    "\n",
    "    def load_progress(self) -> Dict:\n",
    "        \"\"\"加载保存的进度\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            try:\n",
    "                with open(self.checkpoint_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    # 确保数据正确转换为集合\n",
    "                    return {\n",
    "                        'processed_creators': set(data.get('processed_creators', [])),\n",
    "                        'current_creator': data.get('current_creator'),\n",
    "                        'processed_models': set(data.get('processed_models', [])),\n",
    "                        'last_update': data.get('last_update'),\n",
    "                        'total_models': data.get('total_models', 0)\n",
    "                    }\n",
    "            except (json.JSONDecodeError, FileNotFoundError):\n",
    "                print(\"Error loading progress file, starting fresh\")\n",
    "                # 备份损坏的文件\n",
    "                if os.path.exists(self.checkpoint_file):\n",
    "                    backup_file = f\"{self.checkpoint_file}.{datetime.now().strftime('%Y%m%d_%H%M%S')}.bak\"\n",
    "                    shutil.copy2(self.checkpoint_file, backup_file)\n",
    "                return self._get_default_progress()\n",
    "        return self._get_default_progress()\n",
    "\n",
    "    def _get_default_progress(self):\n",
    "        \"\"\"获取默认进度状态\"\"\"\n",
    "        return {\n",
    "            'processed_creators': set(),\n",
    "            'current_creator': None,\n",
    "            'processed_models': set(),\n",
    "            'last_update': None,\n",
    "            'total_models': 0\n",
    "        }\n",
    "\n",
    "    def save_progress(self):\n",
    "        \"\"\"保存当前进度\"\"\"\n",
    "        current_progress = {\n",
    "            'processed_creators': list(self.progress['processed_creators']),\n",
    "            'current_creator': self.progress['current_creator'],\n",
    "            'processed_models': list(self.progress['processed_models']),\n",
    "            'last_update': datetime.now().isoformat(),\n",
    "            'total_models': self.progress['total_models']\n",
    "        }\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(current_progress, f, indent=2)\n",
    "\n",
    "    def mark_creator_complete(self, creator: str, models_count: int):\n",
    "        \"\"\"标记创作者为已完成\"\"\"\n",
    "        self.progress['processed_creators'].add(creator)\n",
    "        self.progress['current_creator'] = None\n",
    "        self.progress['total_models'] += models_count\n",
    "        self.save_progress()\n",
    "\n",
    "    def is_creator_processed(self, creator: str) -> bool:\n",
    "        \"\"\"检查创作者是否已处理\"\"\"\n",
    "        return creator in self.progress['processed_creators']\n",
    "\n",
    "    def update_current(self, creator: str):\n",
    "        \"\"\"更新当前处理的创作者\"\"\"\n",
    "        self.progress['current_creator'] = creator\n",
    "        self.save_progress()\n",
    "\n",
    "    def add_processed_model(self, model_id: int):\n",
    "        \"\"\"添加已处理的模型ID\"\"\"\n",
    "        if model_id not in self.progress['processed_models']:\n",
    "            self.progress['processed_models'].add(model_id)\n",
    "            self._auto_save()\n",
    "\n",
    "    def _auto_save(self):\n",
    "        \"\"\"自动保存（每100个模型保存一次）\"\"\"\n",
    "        if len(self.progress['processed_models']) % 100 == 0:\n",
    "            self.save_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CivitaiModelCrawler:\n",
    "    def __init__(self, api_key: Optional[str] = None, retry_delay: int = 2):\n",
    "        self.base_url = \"https://civitai.com/api/v1\"\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "        if api_key:\n",
    "            self.headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "\n",
    "        self.retry_delay = retry_delay\n",
    "        self.rate_limiter = RateLimitManager()\n",
    "        self.logger = self._setup_logger()\n",
    "\n",
    "        # 文件路径\n",
    "        self.models_file = os.path.join(DATA_DIRS['models'], 'all_models.json')\n",
    "        self.models_csv = os.path.join(DATA_DIRS['csv'], 'all_models.csv')\n",
    "        self.versions_csv = os.path.join(DATA_DIRS['csv'], 'all_versions.csv')\n",
    "\n",
    "        # 加载现有数据\n",
    "        self.all_models = self.load_existing_models()\n",
    "        self.processed_models = {model['id'] for model in self.all_models}\n",
    "\n",
    "        # 并发控制\n",
    "        self.semaphore = asyncio.Semaphore(5)\n",
    "\n",
    "    def _setup_logger(self):\n",
    "        \"\"\"设置日志记录器\"\"\"\n",
    "        logger = logging.getLogger('CivitaiModelCrawler')\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "        if not logger.handlers:\n",
    "            # 文件处理器\n",
    "            fh = logging.FileHandler(\n",
    "                os.path.join(DATA_DIRS['logs'], f'models_crawler_{datetime.now().strftime(\"%Y%m%d\")}.log')\n",
    "            )\n",
    "            fh.setLevel(logging.INFO)\n",
    "\n",
    "            # 控制台处理器\n",
    "            ch = logging.StreamHandler()\n",
    "            ch.setLevel(logging.INFO)\n",
    "\n",
    "            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "            fh.setFormatter(formatter)\n",
    "            ch.setFormatter(formatter)\n",
    "\n",
    "            logger.addHandler(fh)\n",
    "            logger.addHandler(ch)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def load_existing_models(self) -> List[Dict]:\n",
    "        \"\"\"加载现有的模型数据\"\"\"\n",
    "        if os.path.exists(self.models_file):\n",
    "            try:\n",
    "                with open(self.models_file, 'r', encoding='utf-8') as f:\n",
    "                    models = json.load(f)\n",
    "                self.logger.info(f\"Successfully loaded {len(models)} existing models\")\n",
    "                return models\n",
    "            except json.JSONDecodeError:\n",
    "                self.logger.error(\"Error reading models file\")\n",
    "                # 创建备份\n",
    "                backup_file = f\"{self.models_file}.{datetime.now().strftime('%Y%m%d_%H%M%S')}.bak\"\n",
    "                if os.path.exists(self.models_file):\n",
    "                    shutil.copy2(self.models_file, backup_file)\n",
    "                    self.logger.info(f\"Backed up corrupted file to {backup_file}\")\n",
    "                return []\n",
    "        return []\n",
    "\n",
    "    async def _batch_request(self, urls: List[str]) -> List[Dict]:\n",
    "        \"\"\"并发请求多个URL\"\"\"\n",
    "        async def fetch_url(url):\n",
    "            async with self.semaphore:\n",
    "                return await self._make_request(url)\n",
    "\n",
    "        tasks = [fetch_url(url) for url in urls]\n",
    "        return await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    async def _make_request(self, url: str, params: Dict = None) -> Dict:\n",
    "        max_retries = 5\n",
    "        base_delay = self.retry_delay\n",
    "        exponential_base = 2\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                async with aiohttp.ClientSession() as session:\n",
    "                    async with session.get(url, headers=self.headers, params=params) as response:\n",
    "                        if response.status == 429:\n",
    "                            retry_after = response.headers.get('Retry-After')\n",
    "                            wait_time = int(retry_after) if retry_after else base_delay * (exponential_base ** attempt)\n",
    "                            self.logger.warning(f\"Rate limited. Waiting {wait_time} seconds...\")\n",
    "                            await asyncio.sleep(wait_time)\n",
    "                            continue\n",
    "\n",
    "                        if response.status == 403:\n",
    "                            self.logger.error(\"API access forbidden. Check API key.\")\n",
    "                            raise Exception(\"API access forbidden\")\n",
    "\n",
    "                        if response.status == 404:\n",
    "                            self.logger.warning(f\"Resource not found: {url}\")\n",
    "                            return None\n",
    "\n",
    "                        if response.status >= 500:\n",
    "                            wait_time = base_delay * (exponential_base ** attempt)\n",
    "                            self.logger.warning(f\"Server error ({response.status}). Waiting {wait_time} seconds...\")\n",
    "                            await asyncio.sleep(wait_time)\n",
    "                            continue\n",
    "\n",
    "                        response.raise_for_status()\n",
    "                        return await response.json()\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Request error: {str(e)}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                await asyncio.sleep(base_delay * (exponential_base ** attempt))\n",
    "\n",
    "        return None\n",
    "\n",
    "    async def collect_models(self, creator_username: str, start_page: int = 1,\n",
    "                           progress_tracker: Optional[ProgressTracker] = None) -> None:\n",
    "        self.logger.info(f\"Starting to collect models for creator: {creator_username}\")\n",
    "        creator_models = []\n",
    "        batch_size = 100\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                await self.rate_limiter.wait_if_needed()\n",
    "\n",
    "                params = {\n",
    "                    \"username\": creator_username,\n",
    "                    \"page\": start_page,\n",
    "                    \"limit\": 100\n",
    "                }\n",
    "\n",
    "                response_data = await self._make_request(f\"{self.base_url}/models\", params)\n",
    "                if not response_data or 'items' not in response_data:\n",
    "                    break\n",
    "\n",
    "                models = response_data.get('items', [])\n",
    "                if not models:\n",
    "                    break\n",
    "\n",
    "                metadata = response_data.get('metadata', {})\n",
    "                total_items = metadata.get('totalItems', 0)\n",
    "                self.logger.info(f\"Found {total_items} total models for creator {creator_username}\")\n",
    "\n",
    "                # 过滤新模型\n",
    "                new_models = [\n",
    "                    model for model in models\n",
    "                    if model.get('id') and model.get('id') not in self.processed_models\n",
    "                    and model.get('creator', {}).get('username') == creator_username\n",
    "                ]\n",
    "\n",
    "                if not new_models:\n",
    "                    break\n",
    "\n",
    "                # 批量获取模型详情\n",
    "                model_urls = [f\"{self.base_url}/models/{model['id']}\" for model in new_models]\n",
    "                batch_results = await self._batch_request(model_urls)\n",
    "                valid_models = [\n",
    "                    model for model in batch_results\n",
    "                    if isinstance(model, dict) and model.get('id')\n",
    "                ]\n",
    "\n",
    "                if valid_models:\n",
    "                    creator_models.extend(valid_models)\n",
    "                    self.processed_models.update(model['id'] for model in valid_models)\n",
    "                    if progress_tracker:\n",
    "                        for model in valid_models:\n",
    "                            progress_tracker.add_processed_model(model['id'])\n",
    "                    self._save_batch(valid_models)\n",
    "\n",
    "                self.logger.info(f\"Processed {len(creator_models)} models for {creator_username}\")\n",
    "\n",
    "                if not metadata.get('nextPage'):\n",
    "                    break\n",
    "\n",
    "                start_page += 1\n",
    "                await asyncio.sleep(1)\n",
    "\n",
    "            return creator_models\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error collecting models for {creator_username}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _save_batch(self, new_models: List[Dict]):\n",
    "        \"\"\"保存一批新模型\"\"\"\n",
    "        if not new_models:\n",
    "            return\n",
    "\n",
    "        # 将新模型添加到集合\n",
    "        self.all_models.extend(new_models)\n",
    "\n",
    "        # 保存到JSON和CSV\n",
    "        self._save_json()\n",
    "        self._export_batch_to_csv(new_models)\n",
    "\n",
    "    def _save_json(self):\n",
    "        \"\"\"保存JSON数据，保持现有数据\"\"\"\n",
    "        # 读取现有数据\n",
    "        existing_models = []\n",
    "        if os.path.exists(self.models_file):\n",
    "            try:\n",
    "                with open(self.models_file, 'r', encoding='utf-8') as f:\n",
    "                    existing_models = json.load(f)\n",
    "                self.logger.info(f\"Loaded {len(existing_models)} existing models for merging\")\n",
    "            except json.JSONDecodeError:\n",
    "                self.logger.error(\"Error reading existing models file\")\n",
    "                backup_file = f\"{self.models_file}.{datetime.now().strftime('%Y%m%d_%H%M%S')}.bak\"\n",
    "                if os.path.exists(self.models_file):\n",
    "                    shutil.copy2(self.models_file, backup_file)\n",
    "                    self.logger.info(f\"Backed up corrupted file to {backup_file}\")\n",
    "\n",
    "        # 合并去重\n",
    "        existing_ids = {model.get('id') for model in existing_models}\n",
    "        new_models = [model for model in self.all_models if model.get('id') not in existing_ids]\n",
    "        merged_models = existing_models + new_models\n",
    "\n",
    "        # 保存合并后的数据\n",
    "        with open(self.models_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(merged_models, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        self.logger.info(f\"Saved {len(merged_models)} models ({len(new_models)} new) to JSON\")\n",
    "\n",
    "    def _export_batch_to_csv(self, models: List[Dict]):\n",
    "        \"\"\"导出新模型到CSV\"\"\"\n",
    "        if not models:\n",
    "            return\n",
    "\n",
    "        models_data = []\n",
    "        versions_data = []\n",
    "\n",
    "        for model in models:\n",
    "            model_base = {\n",
    "                'id': model.get('id'),\n",
    "                'name': model.get('name'),\n",
    "                'description': model.get('description'),\n",
    "                'type': model.get('type'),\n",
    "                'nsfw': model.get('nsfw'), # boolean\n",
    "                'model_poi': model.get('poi'),\n",
    "                'tags': ','.join(model.get('tags', [])),\n",
    "                'creator': model.get('creator', {}).get('username'),\n",
    "                'stats_download': model.get('stats', {}).get('downloadCount'),\n",
    "                'stats_comment_count': model.get('stats', {}).get('commentCount'),\n",
    "                'stats_favorite_count': model.get('stats', {}).get('favoriteCount'),\n",
    "                'stats_rating_count': model.get('stats', {}).get('ratingCount'),\n",
    "                'stats_rating': model.get('stats', {}).get('rating')\n",
    "            }\n",
    "            models_data.append(model_base)\n",
    "\n",
    "            for version in model.get('modelVersions', []):\n",
    "                version_data = {\n",
    "                    'model_id': model.get('id'),\n",
    "                    'version_id': version.get('id'),\n",
    "                    'version_name': version.get('name'),\n",
    "                    'version_description': version.get('description'),\n",
    "                    'created_at': version.get('createdAt'),\n",
    "                    'updated_at': version.get('updatedAt'),\n",
    "                    'download_url': version.get('downloadUrl'),\n",
    "                    'training_words': ','.join(version.get('trainedWords', [])),\n",
    "                    'base_model': version.get('baseModel')\n",
    "                }\n",
    "                versions_data.append(version_data)\n",
    "\n",
    "        # 追加到CSV文件\n",
    "        write_header = not os.path.exists(self.models_csv)\n",
    "        pd.DataFrame(models_data).to_csv(\n",
    "            self.models_csv,\n",
    "            mode='a',\n",
    "            header=write_header,\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        write_header = not os.path.exists(self.versions_csv)\n",
    "        pd.DataFrame(versions_data).to_csv(\n",
    "            self.versions_csv,\n",
    "            mode='a',\n",
    "            header=write_header,\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        self.logger.info(f\"Exported {len(models_data)} models and {len(versions_data)} versions\")\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"获取统计信息\"\"\"\n",
    "        if not self.all_models:\n",
    "            return {\n",
    "                'total_models': 0,\n",
    "                'unique_creators': 0,\n",
    "                'model_types': Counter(),\n",
    "                'avg_downloads': 0\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'total_models': len(self.all_models),\n",
    "            'unique_creators': len({\n",
    "                model.get('creator', {}).get('username')\n",
    "                for model in self.all_models\n",
    "                if model.get('creator')\n",
    "            }),\n",
    "            'model_types': Counter(\n",
    "                model.get('type')\n",
    "                for model in self.all_models\n",
    "                if model.get('type')\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def validate_data(self):\n",
    "        \"\"\"验证数据完整性\"\"\"\n",
    "        # 检查文件\n",
    "        files_to_check = {\n",
    "            'models_json': self.models_file,\n",
    "            'models_csv': self.models_csv,\n",
    "            'versions_csv': self.versions_csv\n",
    "        }\n",
    "\n",
    "        for name, filepath in files_to_check.items():\n",
    "            if os.path.exists(filepath):\n",
    "                size = os.path.getsize(filepath) / (1024 * 1024)  # MB\n",
    "                mtime = datetime.fromtimestamp(os.path.getmtime(filepath))\n",
    "                print(f\"\\n{name}:\")\n",
    "                print(f\"  路径: {filepath}\")\n",
    "                print(f\"  大小: {size:.2f} MB\")\n",
    "                print(f\"  最后修改: {mtime}\")\n",
    "            else:\n",
    "                print(f\"\\n{name} 不存在: {filepath}\")\n",
    "\n",
    "        # 检查模型数据\n",
    "        try:\n",
    "            with open(self.models_file, 'r', encoding='utf-8') as f:\n",
    "                models = json.load(f)\n",
    "                print(f\"\\n模型数据:\")\n",
    "                print(f\"  总数: {len(models)}\")\n",
    "                print(f\"  创作者数: {len({m.get('creator', {}).get('username') for m in models})}\")\n",
    "                print(f\"  类型分布: {Counter(m.get('type') for m in models)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n读取模型数据失败: {e}\")\n",
    "\n",
    "        # 检查CSV数据\n",
    "        try:\n",
    "            if os.path.exists(self.models_csv):\n",
    "                df_models = pd.read_csv(self.models_csv)\n",
    "                print(f\"\\nCSV数据:\")\n",
    "                print(f\"  模型数: {len(df_models)}\")\n",
    "                print(f\"  创作者数: {df_models['creator'].nunique()}\")\n",
    "                print(f\"  类型分布: {df_models['type'].value_counts().to_dict()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n读取CSV数据失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"主执行函数\"\"\"\n",
    "    try:\n",
    "        print(\"Starting the Civitai crawler...\")\n",
    "\n",
    "        # 初始化进度追踪器和爬虫\n",
    "        progress = ProgressTracker(DATA_DIRS['checkpoints'])\n",
    "        # creator_crawler = CivitaiCreatorsCrawler()\n",
    "        model_crawler = CivitaiModelCrawler()\n",
    "\n",
    "        # 加载或收集创作者数据\n",
    "        # creators_file = os.path.join(DATA_DIRS['creators'], 'creators_completed.json')\n",
    "        # if not os.path.exists(creators_file):\n",
    "        #     print(\"No creators data found. Running creator crawler first...\")\n",
    "        #     await creator_crawler.collect_creators()\n",
    "\n",
    "        # 读取创作者数据\n",
    "        creators_file = os.path('/content/drive/MyDrive/creators_completed.json')\n",
    "        with open(creators_file, 'r') as f:\n",
    "            creators_data = json.load(f)\n",
    "\n",
    "        # 获取活跃创作者\n",
    "        active_creators = [\n",
    "            c['username'] for c in creators_data\n",
    "            if c.get('modelCount', 0) > 0\n",
    "        ]\n",
    "        print(f\"Found {len(active_creators)} active creators\")\n",
    "\n",
    "        # 如果有上次未完成的创作者，从那里开始\n",
    "        if progress.progress['current_creator']:\n",
    "            try:\n",
    "                start_idx = active_creators.index(progress.progress['current_creator'])\n",
    "                active_creators = active_creators[start_idx:]\n",
    "                print(f\"Resuming from creator: {progress.progress['current_creator']}\")\n",
    "            except ValueError:\n",
    "                print(\"Previous creator not found, starting from beginning\")\n",
    "\n",
    "        # 处理每个创作者\n",
    "        total_creators = len(active_creators)\n",
    "        for idx, creator in enumerate(active_creators, 1):\n",
    "            if progress.is_creator_processed(creator):\n",
    "                print(f\"Skipping already processed creator: {creator}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nProcessing creator: {creator} ({idx}/{total_creators})\")\n",
    "            try:\n",
    "                # 收集模型\n",
    "                creator_models = await model_crawler.collect_models(\n",
    "                    creator_username=creator,\n",
    "                    progress_tracker=progress\n",
    "                )\n",
    "\n",
    "                if creator_models:\n",
    "                    progress.mark_creator_complete(creator, len(creator_models))\n",
    "                    print(f\"Completed creator {creator} with {len(creator_models)} models\")\n",
    "                else:\n",
    "                    progress.mark_creator_complete(creator, 0)\n",
    "                    print(f\"No new models for creator {creator}\")\n",
    "\n",
    "                # 打印当前统计\n",
    "                stats = model_crawler.get_stats()\n",
    "                print(\"\\nCurrent Statistics:\")\n",
    "                print(f\"Total Models: {stats['total_models']}\")\n",
    "                print(f\"Unique Creators: {stats['unique_creators']}\")\n",
    "                print(f\"Model Types: {dict(stats['model_types'])}\")\n",
    "\n",
    "                # 创作者之间添加延迟\n",
    "                await asyncio.sleep(5)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing creator {creator}: {e}\")\n",
    "                progress.update_current(creator)\n",
    "                await asyncio.sleep(30)  # 错误后等待更长时间\n",
    "                continue\n",
    "\n",
    "        print(\"\\nCrawling completed!\")\n",
    "\n",
    "        # 验证最终数据\n",
    "        model_crawler.validate_data()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置 Colab 环境\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # 创建事件循环并运行\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(main())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
